<div top-nav></div>

<section class="projects">

	<div class="project-container">
		<div class="project-col ui-widget-content">
			<h1 class="project-title ui-widget-header">Objectives</h1>

			<p class="project-text">GiantSteps seeks to overcome the limitations of current digital music production tools, to create more powerful, inspirational, collaborative, affordable, user-friendly, and efficient music creation systems. In order to achieve this, there are three scientific and technological objectives to be addressed:</p>

			<p><strong class="project-header">Objective 1: Develop High-Performance and Low-Complexity Methods for Music Analysis and Multimodal Music Information Research</strong></p>

			<p>Extracting musical parameters related to rhythm, tempo, melody, or harmony directly from a digital audio signal plays a central role both in music information research (MIR) and advanced music applications. While in academic research the primary goal is to push the limits of what can theoretically be done, for market-ready products, the priority is on delivering robust solutions that work satisfyingly on the target user’s hardware configuration. The central challenge within this objective is to <strong>develop cutting edge MIR technology while bridging the gap with end users’ requirements</strong> to allow for better integration of research outcomes into real world systems.</p>

			<p>To this end, research into <strong>content-based MIR</strong>, with respect to <strong>description of rhythm, tempo, timbre, harmony, melody, and musical structure</strong> is carried out. In addition, further descriptors useful in the context of music production such as “grooveness” (indicating the likeliness that the beat pattern will evoke rhythmical movement from the listener), “loopability” (indicating how well a sample or a segment thereof is suited to be used in a loop), or “predictability” (as opposed to musical complexity) may be developed. For every descriptor, emphasis is put on high efficiency in terms of:</p>

			<ul>
				<li><strong>Complexity:</strong> With a large application potential for affordable hardware solutions, such as smartphones and tablets, algorithms need to be designed to work on platforms that may offer only limited resources in terms of CPU speed, CPU instructions, and memory. Nevertheless, developed algorithms are expected to extract musical properties <strong>faster than real-time</strong> even in environments scarce of these resources.</li>
				

				<li><strong>Effectiveness:</strong> Even for current algorithms for extraction of musical properties and knowledge there is still room for improvements with the specific use cases in mind. We will develop new and more effective algorithms for meaningful description of music data in the targeted application scenarios. This will be done by focusing not just on the general accuracy of the algorithms, but on their <strong>effectiveness for the tasks potential users will perform</strong> (application and task oriented MIR algorithms).</li>


				<li><strong>Scalability:</strong> In order to allow the <strong>applicability of the developed methods on massive collections of material</strong> (which may range from the millions of sound samples considered normal today to unknown future quantities) easily indexable feature representations will be chosen.</li>
			</ul>

			<p>In addition to the musical information extracted directly from the audio content, <strong>further musical knowledge will be extracted from the richest source of musical context, i.e. the web</strong>. The web contains plenty of music related texts that describe even the most specific musical styles and their characteristics. We tap this knowledge of <strong>semantic music descriptions</strong> by developing new, ontology and domain knowledge driven methods for web-based Music Information Extraction.</p>

			<p><strong>Measurable outcomes will include:</strong></p>

			<ul>
				<li>Improved descriptors for rhythm, tempo, timbre, harmony, and melody.</li>
				<li>Optimized structure detection algorithms.</li>
				<li>Optimized extraction algorithms for low resource platforms.</li>
				<li>Methods for automatically extracting stylistic definitions from semi-structured web sources and mapping of extracted definitions to musical and acoustic properties.</li>
			</ul>

			<p>These outcomes will be measured by evaluating their effectiveness for the tasks they will be included in, as well as testing run-time performance and usage of resources on a variety of hardware configurations and systems. The steps necessary to achieve this objective will be carried out in WP3. The resulting software components and the derived knowledge will be used by the expert and recommendation systems (objective 2) and for visualizing musical properties (objective 3).</p>


			<p><strong class="project-header">Objective 2: Create Audio and Music Expert Systems to Guide Users in Composition</strong></p>


			<p>Audio and music experts will bring theoretical musical knowledge, typically encoded in symbolic notations and compositional theories, to users accustomed only to work with sound loops and files, thus actively promoting the use of advanced musical concepts. These agents will use and combine the musical knowledge contained in the users’ pool of audio material, which will has been extracted following objective 1, for inferring meaningful music information, identifying compositional issues, and suggesting variations, the use of alternative material, developments or new musical solutions. Five types of expert agents will be developed, each of them dealing with higher-level musical knowledge, and respectively concerned with: (1) sample and loop based segmentation; (2) melody and rhythm; (3) harmony and chord progression; (4) traditional song and form structures identification and segmentation; and (5) genre/style suitability.</p>

			<p><strong>Sample and loop-based segmentation agents</strong> will identify the separated audio parts and sources being used in a composition and detect new potential cutting points as well.<strong>Melody and rhythm agents</strong> will be able to suggest, based on the audio material being used (as well as on the previous symbolic material having been already suggested and integrated), new melodic and/or rhythmic sequences, together with variations of the existing ones. <strong>Harmony and chord progression agents</strong> will infer the key (e.g. fundamental + major or minor) and the chords being used, for suggesting successive or alternative chord progressions. <strong>Structure identification and segmentation agents</strong> will be able to identify important parts in the users’ compositions for suggesting repetitions, variations, or the inclusion of new complementary parts. Finally, the <strong>genre/style agent</strong> will give specific advice especially suited for the style of the music being produced. To this end, musical knowledge about specific styles will be expressed in symbolic form. On top of that, the semantic knowledge about musical styles derived from the web will allow adapting compositions to further, even more specific styles and enabling the agent to automatically stay up-to-date on new musical developments expressed and documented on the web.</p>

			<p>These expert agents will be developed combining two complementary strategies, namely heuristic and AI (machine learning) based approaches. The completion of this objective will require the integration of disparate knowledge and strategies that will push further the current state of the art in this area.</p>

			<p><strong>Measurable outcomes will include:</strong></p>

			<ul>
				<li>Melody and rhythm software components that will facilitate the generation and the variation of melodies and rhythmic phrases.</li>
				<li>Harmony and chord progression software components that will detect key and tonality, suggest alternative or following chords, and correct/adapt existing melodies to alternative chords.</li>
				<li>Song structure identification and recommendation software components that will identify relevant parts in a song, and suggest complementary or alternative structures.</li>
				<li>Sample, loop, and song recommendation software components, that will help identifying new material to be introduced in the piece as well as their fitting positions, or new tracks to be played after the current one.</li>
				<li>Genre/style suitability software components, which will recommend stylistic modifications and material based on an aimed style (both, based on explicit musical knowledge and knowledge automatically derived from the web and accessible through natural language requests).</li>
			</ul>

			<p>This will be <strong>measured by</strong> human assessment from both music experts and non-experts. The steps necessary to achieve this objective will be carried out in <strong>WP4</strong>. The software components will make use of the outcome descriptors resulting from objective 1, and will on its turn be used by the interfaces developed in objective 3.</p>


			<p><strong class="project-header">Objective 3: Develop Natural and Powerful Visual and Tangible Interfaces for on- and off-site Collaboration</strong></p>

			<p>No matter how well grounded or wise they can be, artificial knowledge and expert agent-based advices might be completely useless or, even worse, annoying and odious, if they are not expressed in the right forms. We consider the seamless integration of musical knowledge into the creative process of the users in order to truly preserve and support their modus operandi and flow one of the central objectives of the project. As a result, the development of powerful and yet transparent, natural and “flow friendly” interfaces is an essential challenge in GiantSteps. The development of novel assistive and intelligent music expert agents calls for new concepts in user interaction and user interfaces to reflect these unprecedented possibilities.</p>

			<p>Matching the consortium’s structure and partners for exploitation, we will pursue a two-way strategy in terms of user interface innovation. On one hand we will address the more “classical” production workflow based on a block-building metaphor as incorporated in many currently available commercial music production software packages (such as Native Instruments’ software suite Maschine). On the other hand we will develop new tangible interfaces that will allow for real-time hands-on production and performance, in the tradition of the Reactable. For both these production scenarios we will also develop interaction metaphors well suited for affordable but small multi-touch devices such as smartphones and tablets. Collaborative issues will also be specially addressed. Onsite collaboration will be covered with tabletop tangible interfaces, as well as with distributed interfaces for portable devices. Online collaboration will be covered with the development of cloud-based technologies for distributed non-real-time music production.</p>

			<p>In addition, including novel visualization techniques that provide meaningful feedback is crucial to further facilitate usability in all the interfaces we will work with. We will pursue research beyond the state of the art in non-obtrusive visualization of musical parameters in complex production scenarios, in new HCI paradigms for musical interaction and control of multi-dimensional parameter spaces, and in the manipulative representation of complex hierarchical structures (such as songs). We do also expect that several of these new HCI paradigms will be reused in other real-time control contexts not related to music.</p>

			<p><strong>Measurable outcomes will include:</strong></p>

			<ul>
				<li>Widgets for integrating new audio and musical knowledge and advisors into production suite user interfaces.</li>
				<li>Interactive visualization techniques for integrating new audio and musical knowledge and advisors into multi-touch mobile devices.</li>
				<li>Interactive visualization techniques for song structure and tonal spaces.</li>
				<li>Multi-user distributed interfaces for local network collaborative work using mobile devices and/or tangible interfaces in sync.</li>
				<li>Multi-user distributed interfaces for remote, cloud-based collaborative work without real-time requirement.</li>
			</ul>


			<p>These outcomes will be <strong>measured by</strong> usability tests. The steps necessary to achieve this objective will be carried out in <strong>WP5</strong>. The results of this objective will be presented to and tested with users (WP2).</p>
		</div>
		<div class="project-col">
			<h1 class="project-title">Work Plan</h1>

			<img src="images/pert.svg" />

			<p><strong class="project-header">WP1 – Project Management (JCP-Consult)</strong></p>
			<p>WP1 activities address the overall coordination and management of the project. A suitable structure is in place in order to guarantee fulfilment of the project objectives. It covers high-level issues and daily management, as well as the integration of regular daily activities and the setup and implementation of efficient coordination and management of the overall project.</p>

			<p><strong class="project-header">WP2 – Iterative user-centered Design & Evaluation (STEIM)</strong></p>
			<p>WP2 focuses on the requirements of the user-driven development process and the testing of prototypes with end-users. Thus, in the first step, the methodological foundations of GiantSteps will be defined. Throughout the project, WP2 includes the planning and organization of workshops and Red Bull Music Academies, the conducting of user experiments in real use scenarios, and the analysis of the obtained feedback. WP2 will inform WP4 (“Audio & Music Experts”) and WP5 (“Interaction”) of all results gathered during experimentation in order to refine their functionality. Information gathered through WP5 will in turn feed back into WP2.</p>

			<p><strong class="project-header">WP3 – Audio & Music Knowledge Extraction (JKU)</strong></p>
			<p>WP3 is the core work package to realize the S/T Objective 1 of the project, i.e., to develop highperformance and low-complexity methods for music analysis and multimodal Music Information Research. The musical knowledge extracted here will feed into WP4, where the extracted knowledge is integrated, WP5, where the extracted parameters are used for visualization of musical properties, and WP6 (“Product Integration”). The requirements of WP4 will also influence the developments to be made in this WP. Outcomes will be disseminated in academic contexts (WP7).</p>

			<p><strong class="project-header">WP4 – Audio & Music Experts (UPF-MTG)</strong></p>
			<p>The goal of WP4 is to develop supportive and inspirational higher-level music expert agents for melody, harmony, and rhythm composition, as well as structural and stylistic agents, that assist users in the creation process. Developed expert agents will transparently guide users when they lack relevant technical or musical knowledge. This is aligned with the S/T Objective 2 of the project. In order to develop these expert agents that conform to the findings of WP2, a combination of heuristic and machine learning strategies will be applied to the knowledge extracted in WP3 (or knowledge assumed to be extractable in order to make development in WP4 less dependent on WP3). Developments of WP4 will be integrated into WP5 and WP6. Outcomes will be disseminated in academic contexts (WP7).</p>

			<p><strong class="project-header">WP5 – Interaction, interfaces, visualization, collaboration (REACTABLE)</strong></p>
			<p>WP5 addresses the development of the user interfaces that will make use of all the musical knowledge gathered from the algorithms developed in WP3 and WP4, fulfilling S/T Objective 3 of the project. This comprises the development of novel widgets for DAWs, novel visualization techniques. to display of musical knowledge, real-time visualization and interaction paradigms for mobile devices, and multiuser distributed interfaces for collaborative work (on-site and on-line) on mobile devices and/or tangible interfaces. Interface prototypes will be evaluated with users (WP2). Outcomes will be disseminated in academic contexts, as well as through workshops and concerts</p>

			<p><strong class="project-header">WP6 – Product Integration (NI)</strong></p>
			<p>WP6 deals with product integration of the developed technology into existing solutions of the industrial partners as well as into new tools, applications, and Apps. Outcomes will be disseminated in industrial contexts, as well as through workshops and concerts (WP7).</p>

			<p><strong class="project-header">WP7 – Dissemination and Exploitation (NI)</strong></p>
			<p>WP7 is concerned with the dissemination of the project’s results on various levels. Apart from dissemination material such as posters, flyers, a project website, a newsletter, or a social network profile, that will also aid in connecting with the community and related EC projects, the main dissemination strategies comprise the academic dissemination of results at conferences or through journals and the industrial dissemination (e.g., through promotions at expositions or by conceiving a business model and exploitation plan). The foundations of a standardization activity will be put in place based on the developed protocol and APIs. Furthermore, the organization of workshops, concerts and events such as Music Hack Days, together with the presence at the Red Bull Music Academies, will ensure a large visibility and outreach of the project.</p>


		</div>
		<div class="project-col">
			<h1 class="project-title">Consortium</h1>

		</div>

	</div>
</section>

<nav-bar display="fixed"></nav-bar>